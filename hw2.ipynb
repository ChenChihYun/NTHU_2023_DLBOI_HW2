{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li0bVCTuxc6n"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "### Fall 2023\n",
        "\n",
        "#### 11210IPT 553000\n",
        "\n",
        "#### Deep Learning in Biomedical Optical Imaging\n",
        "\n",
        "## Homework 2\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9gtdF75U93qD"
      },
      "source": [
        "### ✏️ Task A: Transitioning to Cross-Entropy Loss (20 pts)\n",
        "\n",
        "In Lab, we utilized the **Binary Cross-Entropy (BCE) Loss** for a binary classification task. The BCE loss is articulated as:\n",
        "\n",
        "$$ \\text{BCE}(y, \\hat{y}) = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right) $$\n",
        "\n",
        "Here, $y$ is the true label (0 or 1), and $\\hat{y}$ denotes the predicted probability of $y=1$.\n",
        "\n",
        "In this task, we aim to explore the implementation of a model using **Cross-Entropy (CE) Loss**, which is a more common approach for classification tasks, especially when dealing with multiple classes. CE loss is expressed as:\n",
        "\n",
        "$$ \\text{CE}(y, \\hat{y}) = -\\sum_{i} y^{(i)} \\log(\\hat{y}^{(i)}) $$\n",
        "\n",
        "In this expression, $y$ represents the ground truth labels, $ \\hat{y} $ is the predictions from your model, and $i$ is the index of the class.\n",
        "\n",
        "\n",
        "#### 1. Modify the Loss (3 pts)\n",
        "Transition to using Cross-Entropy (CE) Loss for the classification task by utilizing PyTorch's built-in functionalities. You can refer to the [official PyTorch documentation](https://pytorch.org/docs/stable/nn.html) for detailed information and guidance to ensure the correct implementation of the CE loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzs3WHm993qE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Replace '...' with the appropriate loss function in PyTorch\n",
        "loss = nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD0Vtqt593qF"
      },
      "source": [
        "#### 2. Modify the Model Architecture (2 pts)\n",
        "To adapt the original code for use with Cross-Entropy (CE) loss, make necessary modifications to the model architecture. Ensure it is compatible and optimized for the application of CE loss. Consider the number of output nodes and the activation function used in the output layer for effective multi-class classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP67SU6693qF"
      },
      "outputs": [],
      "source": [
        "# Modifying the architecture to be compatible with CE loss\n",
        "ce_model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256*256*1, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 2), # nn.Linear(256, num_classes) num_classes為類別數量，BCE只有兩類因此只需返回一值,而CE需要返回一個元素數量與類別數量相同的矩陣，而在此數量為2.\n",
        "    nn.Softmax(dim=1)  # (not sure) 用sofmax使輸出為概率分布(0~1)\n",
        ").cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nWzEk0qJ93qF"
      },
      "source": [
        "#### 3. Reflection Questions (15 pts, 5 pts for each)\n",
        "Provide detailed answers to the questions below:\n",
        "\n",
        "**Q1. Loss Function Comparison:**  \n",
        "   What are the differences between Binary Cross-Entropy (BCE) loss and Cross-Entropy (CE) loss?\n",
        "\n",
        "**Q2. Model Architecture Modification:**  \n",
        "   What motivated the specific changes you made to the model architecture?\n",
        "\n",
        "**Q3. Adapting to CE Loss:**  \n",
        "   In the original code configured for BCE loss, two major adjustments are needed for adaptation to CE loss. Analyze and explain the necessity for these changes, referring to the code below.\n",
        "\n",
        "```python\n",
        "for images, labels in train_loader:\n",
        "    images = images.cuda()\n",
        "    images = images / 255.0\n",
        "    labels = labels.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Change #1: Adaptation to the labels for CE loss\n",
        "    labels = labels.long()  # Changed from labels.float().unsqueeze(1) for BCE loss\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Change #2: Predictions for CE loss\n",
        "    train_predicted = outputs.argmax(-1)  # Changed from torch.sigmoid(outputs) > 0.5 for BCE loss\n",
        "    train_correct += (train_predicted == labels).sum().item()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wis11W7993qF"
      },
      "source": [
        "#### Put Your Response Here:\n",
        "\n",
        "##### 1. BCE與CE用於不同分類類型的問題，BCE用於二元分類，而CE適用於多元分類，也包含二元分類。兩者使用上的差異包含其標籤所適用的資料型態以及預測所使用的模型。(於Q3詳細說明) (CE 可解決類別不平衡問題，有空補充)\n",
        "\n",
        "##### 2. (1) Change **nn.Linear(256, 1) to nn.Linear(256, 2)**。nn.Linear(256, num_classes)，其中 num_classes為類別數量，BCE只有兩類因此只需返回一值,而CE需要返回一個元素數量與類別數量相同的矩陣，而在此訓練數量為2.\n",
        "(2) 加入sofmax使輸出為概率分布(0~1)\n",
        "##### 3. 從BCE loss 至 CE loss 需要改變兩個函數的使用方式，分別為 (1) 資料標籤型態 以及 (2) 預測的選擇\n",
        "(1) ***資料標籤型態:***\n",
        "**BCE loss 使用浮點數**來標籤資料，因此使用函數 **labels.float().unsqueeze(1)**；而**CE loss 使用整數**作為資料標籤型態，因此函式改為 **labels.long()**，使用long來將label 的資料型態改為LongTensor.\n",
        "\n",
        "(2) ***Predictions (預測):***\n",
        "**BCE loss** 使用於二元分類，因此回傳的預測是一個值，表示每個樣本屬於正確的概率值為何(0到1之間)。其使用函數**sigmoid 來將模型的輸出 \"output\" 轉換為概率**；而CE loss用於處理多類別問題，因此其需回傳一陣列(向量)，每個元素介於0~1且總和為1。利用**argmax(-1)** 取得**ouput**裡，每個樣本最大機率所屬於的類別。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei0HUT_x93qF"
      },
      "source": [
        "### ✏️ Task B: Creating an Evaluation Code (20 pts)\n",
        "\n",
        "Evaluate the performance of a pretrained deep learning model with a test dataset of chest X-ray images available in `test_normal.npy` and `test_pneumonia.npy` files. These files respectively contain 200 grayscale normal and pneumonia chest X-ray images, each of size 256×256. The objective is to calculate the model’s accuracy, defined as the percentage of images correctly classified. To accomplish this, you are tasked to write code that loads, processes, and evaluates the model on this specific dataset. Ensure each segment of code replacing the `...` placeholders is functional and aligns with the steps provided in the instructions.\n",
        "\n",
        "**Note: ⚠️ Ensure to upload your trained model's weights to your working environment if needed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IakOZZ-Q93qG"
      },
      "source": [
        "### Step 0: Download test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWnj8xOH93qG",
        "outputId": "0f1c9387-fed0-41b5-ba8f-f03d67cf86f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-15 19:21:13--  https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_normal.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13107328 (12M) [application/octet-stream]\n",
            "Saving to: ‘test_normal.npy’\n",
            "\n",
            "test_normal.npy     100%[===================>]  12.50M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-10-15 19:21:14 (161 MB/s) - ‘test_normal.npy’ saved [13107328/13107328]\n",
            "\n",
            "--2023-10-15 19:21:14--  https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_pneumonia.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13107328 (12M) [application/octet-stream]\n",
            "Saving to: ‘test_pneumonia.npy’\n",
            "\n",
            "test_pneumonia.npy  100%[===================>]  12.50M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-10-15 19:21:15 (146 MB/s) - ‘test_pneumonia.npy’ saved [13107328/13107328]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_normal.npy\n",
        "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_pneumonia.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIw-yxgX93qG"
      },
      "source": [
        "### Step 1: Prepare your test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrmC54Nd93qH",
        "outputId": "5dd38109-4024-42bf-982d-0cb5de5ba1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of test_abnormal: (200, 256, 256)\n",
            "Shape of test_normal: (200, 256, 256)\n",
            "Shape of x_test: (400, 256, 256)\n",
            "Shape of y_test: (400,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_abnormal = np.load('test_pneumonia.npy')\n",
        "test_normal = np.load('test_normal.npy')\n",
        "\n",
        "print(f'Shape of test_abnormal: {test_abnormal.shape}')\n",
        "print(f'Shape of test_normal: {test_normal.shape}')\n",
        "\n",
        "# For the data having presence of pneumonia assign 1, for the normal ones assign 0.\n",
        "test_abnormal_labels = np.ones((test_abnormal.shape[0],))\n",
        "test_normal_labels = np.zeros((test_normal.shape[0],))\n",
        "\n",
        "x_test = np.concatenate((test_abnormal, test_normal), axis=0)\n",
        "y_test = np.concatenate((test_abnormal_labels, test_normal_labels), axis=0)\n",
        "\n",
        "print(f'Shape of x_test: {x_test.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghJArswi93qH"
      },
      "source": [
        "### Step 2: Load Test Images into PyTorch DataLoader (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ji0YeoI93qH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "# 以下 x 為資料 y為資料對應的標籤；資料使用float，標籤使用long(int)\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "# Combine the images and labels into a dataset\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create a dataloader to load data in batches. Set batch size to 32.\n",
        "# Create dataloaders #把data 分成 mini batch 分次load\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3J9yYTb93qH"
      },
      "source": [
        "### Step 3: Prepare Your Trained Model  (5 pts)\n",
        "- Define the architecture to match exactly with the trained model intended for inference. Ensure strict alignment to avoid errors during evaluation.\n",
        "- Load the weights from the trained model and set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "ZiOmcXb293qH",
        "outputId": "68f78a21-1fb0-4c2c-bd06-535bc9501e86"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-13a8107ec47e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the trained weights (Loading the Best Model?)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_classification.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.pth pytorch weight 會用的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_classification.pth'"
          ]
        }
      ],
      "source": [
        "# Declare the model architecture (lab3 D.)\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),#把圖片展成1D 65536\n",
        "    nn.Linear(256*256*1, 256), #??\n",
        "    nn.ReLU(), #\n",
        "    nn.Linear(256, 1) # Map回Class ,1: about loss, BCE cross entropy; 作業裡的 CE entropy 要用2\n",
        ").cuda()\n",
        "\n",
        "# Load the trained weights (Loading the Best Model?)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('model_classification.pth'))#.pth pytorch weight 會用的\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAeBUzbM93qH"
      },
      "source": [
        "### Step 4: Perform Inference and Calculate the Accuracy (10 pts)\n",
        "- Ensure the image values are processed in a manner consistent with the training phase.\n",
        "- Use the model that was trained with BCE loss to execute inference on the test dataset.\n",
        "- Note that inference should be performed in GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE92mQx093qH"
      },
      "outputs": [],
      "source": [
        "# Lab2 是隨機抽img 測試\n",
        "# 在此直接測試整個Test data\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images = images.cuda()\n",
        "        images = images / 255.\n",
        "\n",
        "        labels = labels.cuda() # for BCE\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        labels_float = labels.float().unsqueeze(1)  # Convert labels to float and match shape with outputs for BCE\n",
        "        predicted = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        test_correct += (predicted.float() == labels_float).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "train_accuracy = 100. * test_correct / test_total\n",
        "print(f'Test accuracy is {train_accuracy}%.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}